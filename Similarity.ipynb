{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000017471CDADE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "This signature belongs to Chaitali\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('./Signature.h5')\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "test_image = image.load_img('./DATASET/Chaitali/pnew.jpeg', target_size = (64,64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image=test_image/255\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = model.predict(test_image)\n",
    "\n",
    "if result[0]<0:\n",
    "    p1='PES1PG22CS008.txt'\n",
    "    print(\"This signature belongs to Chaitali\")\n",
    "else:\n",
    "    p1='PES1PG22CS021.txt'\n",
    "    print(\"This signature belongs to Kanak\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T13:04:32.672880Z",
     "end_time": "2023-05-05T13:04:33.150071Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess_doc(doc):\n",
    "    # Convert to lowercase\n",
    "    doc = doc.lower()\n",
    "\n",
    "    # Remove special characters and digits\n",
    "    doc = re.sub(r\"[^a-zA-Z]+\", ' ', doc)\n",
    "\n",
    "    # Tokenize document\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Join tokens back into a string\n",
    "    processed_doc = ' '.join(tokens)\n",
    "\n",
    "    return processed_doc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T09:55:37.833807Z",
     "end_time": "2023-05-05T09:55:37.928031Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "txts = dict()\n",
    "for f_name in os.listdir('./TXTs'):\n",
    "    f = open('./TXTs/'+f_name, 'r', encoding='utf8')\n",
    "    temp = preprocess_doc(f.read())\n",
    "    txts[f_name] = temp\n",
    "    f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T09:55:55.190644Z",
     "end_time": "2023-05-05T09:55:57.499175Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between PES1PG22CS008.txt and PES1PG22CS001.txt: 0.09402638599254613\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS002.txt: 0.32241820599478666\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS003.txt: 0.0738974820325245\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS004.txt: 0.010868947527761906\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS005.txt: 0.23416708623271112\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS006.txt: 0.12128814338355105\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS007.txt: 0.08903662205099919\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS008.txt: 0.9999999999999999\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS009.txt: 0.04225040703569146\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS010.txt: 0.10791573804938231\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS011.txt: 0.29597494337980446\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS012.txt: 0.06399413665093934\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS013.txt: 0.31620042288061323\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS014.txt: 0.030490661181214664\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS015.txt: 0.03779592493818542\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS016.txt: 0.23882815525462775\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS017.txt: 0.05747422616359002\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS018.txt: 0.009548241701089253\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS019.txt: 0.1918512999459837\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS020.txt: 0.273260548949508\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS021.txt: 0.9387420203270791\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS022.txt: 0.04321196964300674\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS024.txt: 0.08199588693289385\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS025.txt: 0.31112701272879406\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS026.txt: 0.007652614459428922\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS027.txt: 0.03764501029137379\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS028.txt: 0.2029520555901855\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS029.txt: 0.14380985954601624\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS031.txt: 0.05767302400658313\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS032.txt: 0.02769946863509764\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS034.txt: 0.04851088527623862\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS035.txt: 0.01604913320991537\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS036.txt: 0.29906338518096914\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS037.txt: 0.23587758068483775\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS038.txt: 0.07109860886799577\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS039.txt: 0.18698120536628118\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS040.txt: 0.051346662079227376\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS041.txt: 0.05844913842334612\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS042.txt: 0.09058441906797905\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS043.txt: 0.02641241943143741\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS044.txt: 0.12230978420647215\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS045.txt: 0.1000907679463366\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS046.txt: 0.40206605089649866\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS047.txt: 0.23924463059019258\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS048.txt: 0.038521290114662536\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS049.txt: 0.024872654789180257\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS050.txt: 0.00876299560033894\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS051.txt: 0.04124424210329893\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS052.txt: 0.16247678313751904\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS053.txt: 0.015659125514902894\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS054.txt: 0.1369014881738128\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS055.txt: 0.23985808351265675\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS056.txt: 0.015843906869290932\n",
      "Similarity between PES1PG22CS008.txt and PES1PG22CS057.txt: 0.05671465845119628\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def similarity_vec(doc1, doc2):\n",
    "    # Create TfidfVectorizer and fit on documents\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit_transform([doc1, doc2])\n",
    "\n",
    "    # Compute cosine similarity between documents\n",
    "    doc1_vector = vectorizer.transform([doc1])\n",
    "    doc2_vector = vectorizer.transform([doc2])\n",
    "    similarity = cosine_similarity(doc1_vector, doc2_vector)\n",
    "\n",
    "    return similarity[0][0]\n",
    "\n",
    "# Sample documents\n",
    "doc1 = txts[p1]\n",
    "for i in txts.keys():\n",
    "    doc2 = txts[i]\n",
    "    d2 = i\n",
    "    s = similarity_vec(doc1,doc2)\n",
    "    print(f\"Similarity between {p1} and {d2}: {s}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T09:55:58.942527Z",
     "end_time": "2023-05-05T09:55:59.176875Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PES1PG22CS008\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import  webbrowser\n",
    "print(p1[:-4])\n",
    "webbrowser.open_new_tab('C:/Users/HP/PycharmProjects/pythonProject/PDFs/'+p1[:-4] + '.pdf')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T09:56:11.143783Z",
     "end_time": "2023-05-05T09:56:11.269564Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-01T13:14:15.989615Z",
     "end_time": "2023-05-01T13:14:15.989615Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
